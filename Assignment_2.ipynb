{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zyJ25uz0kSaw"
      },
      "source": [
        "# Assignment 2 on Natural Language Processing\n",
        "\n",
        "### Date : 15th Sept, 2020\n",
        "\n",
        "### Instructor : Prof. Sudeshna Sarkar\n",
        "\n",
        "### Teaching Assistants : Alapan Kuila, Aniruddha Roy, Anusha Potnuru, Uppada Vishnu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ao1nhg9RknmF"
      },
      "source": [
        "The central idea of this assignment is to make you familiar with programming in python and also the language modelling task of natural language processing using the python.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "stk58juYkzEr"
      },
      "source": [
        "**Dataset:** \n",
        "\n",
        " Use the text file provided along."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rT6byv49kdmo",
        "colab": {}
      },
      "source": [
        "# read dataset\n",
        "file= open(\"/content/corpus.txt\", 'r')\n",
        "text=file.read()\n",
        "file.close()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SRGqKaDn1pJy"
      },
      "source": [
        "Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C1OtHn6B1oc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "5c98e3c9-33ac-42f0-c90f-6573302100ec"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import string\n",
        "sentences= sent_tokenize(text)\n",
        "corpus=[]\n",
        "for sentence in sentences:\n",
        "  sentence= sentence.lower()\n",
        "  word= sentence.split()\n",
        "  table=[str.maketrans('','', string.punctuation)]   #removing punctuation by creating a mapping table\n",
        "  temp= [w.translate(table) for w in word]\n",
        "  words=[word for word in temp if word.isalpha()]\n",
        "  sentence=\" \".join(words)\n",
        "  corpus.append(sentence)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YDL7yfpXkMRS"
      },
      "source": [
        "### Task: In this sub-task, you are expected to carry out the following tasks:\n",
        "\n",
        "1. **Create the following language models** on the training corpus: <br>\n",
        "    i.   Unigram <br>\n",
        "    ii.  Bigram <br>\n",
        "    iii. Trigram <br>\n",
        "    iv.  Fourgram <br>\n",
        "\n",
        "2. **List the top 5 bigrams, trigrams, four-grams (with and without Add-1 smoothing).**\n",
        "(Note: Please remove those which contain only articles, prepositions, determiners. For Example: “of the”, “in a”, etc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u3oIulBikPua",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "68b3e2e5-524c-4fb8-a780-abaec2e4bdcf"
      },
      "source": [
        "#Write code\n",
        "\n",
        "from nltk.util import ngrams\n",
        "unigrams=[]\n",
        "bigrams=[]\n",
        "trigrams=[]\n",
        "fourgrams=[]\n",
        "\n",
        "for content in (corpus):\n",
        "    words=word_tokenize(content)\n",
        "    unigrams.extend(words)\n",
        "    bigrams.extend(ngrams(words,2))\n",
        "    ##similar for trigrams and fourgrams\n",
        "    trigrams.extend(ngrams(words,3))\n",
        "    fourgrams.extend(ngrams(words,4))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['chapter', 'down', 'the', 'alice', 'was', 'beginning', 'to', 'get', 'very', 'tired']\n",
            "[('down', 'the'), ('the', 'alice'), ('alice', 'was'), ('was', 'beginning'), ('beginning', 'to'), ('to', 'get'), ('get', 'very'), ('very', 'tired'), ('tired', 'of'), ('of', 'sitting')]\n",
            "[('down', 'the', 'alice'), ('the', 'alice', 'was'), ('alice', 'was', 'beginning'), ('was', 'beginning', 'to'), ('beginning', 'to', 'get'), ('to', 'get', 'very'), ('get', 'very', 'tired'), ('very', 'tired', 'of'), ('tired', 'of', 'sitting'), ('of', 'sitting', 'by')]\n",
            "[('down', 'the', 'alice', 'was'), ('the', 'alice', 'was', 'beginning'), ('alice', 'was', 'beginning', 'to'), ('was', 'beginning', 'to', 'get'), ('beginning', 'to', 'get', 'very'), ('to', 'get', 'very', 'tired'), ('get', 'very', 'tired', 'of'), ('very', 'tired', 'of', 'sitting'), ('tired', 'of', 'sitting', 'by'), ('of', 'sitting', 'by', 'her')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:12: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
            "  if sys.path[0] == '':\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vARsvSfynePr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "5b652a97-350d-47ef-e19c-b5f64633ea11"
      },
      "source": [
        "#stopwords = code for downloading stop words through nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "#print top 10 unigrams, bigrams after removing stopwords\n",
        "uni_processed = [p for p in unigrams if p not in stop_words]\n",
        "fdist = nltk.FreqDist(uni_processed)\n",
        "print(fdist.most_common(10))\n",
        "#print top 10 bigrams, trigrams, fourgrams after removing stopwords\n",
        "bi_processed= [p for p in bigrams if p not in stop_words]\n",
        "biDist=nltk.FreqDist(bi_processed)\n",
        "print(biDist.most_common(10))\n",
        "tri_processed= [p for p in trigrams if p not in stop_words]\n",
        "triDist= nltk.FreqDist(tri_processed)\n",
        "print(triDist.most_common(10))\n",
        "four_processed= [p for p in fourgrams if p not in stop_words]\n",
        "fourDist= nltk.FreqDist(four_processed)\n",
        "print(fourDist.most_common(10))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[('said', 420), ('alice', 221), ('little', 119), ('went', 79), ('one', 78), ('like', 75), ('would', 71), ('thought', 63), ('could', 63), ('mock', 56)]\n",
            "[(('said', 'the'), 208), (('of', 'the'), 134), (('in', 'a'), 98), (('the', 'and'), 94), (('and', 'the'), 82), (('in', 'the'), 79), (('to', 'the'), 73), (('at', 'the'), 61), (('as', 'she'), 60), (('she', 'had'), 57)]\n",
            "[(('the', 'mock', 'turtle'), 31), (('said', 'the', 'mock'), 19), (('of', 'the', 'and'), 17), (('she', 'said', 'to'), 17), (('the', 'and', 'the'), 17), (('one', 'of', 'the'), 15), (('the', 'march', 'hare'), 15), (('said', 'the', 'and'), 14), (('the', 'said', 'the'), 12), (('out', 'of', 'the'), 11)]\n",
            "[(('as', 'well', 'as', 'she'), 6), (('in', 'a', 'tone', 'of'), 6), (('said', 'the', 'mock', 'turtle'), 6), (('the', 'poor', 'little', 'thing'), 5), (('a', 'minute', 'or', 'two'), 5), (('the', 'first', 'said', 'the'), 5), (('the', 'moral', 'of', 'that'), 5), (('she', 'did', 'not', 'like'), 4), (('did', 'not', 'like', 'to'), 4), (('she', 'came', 'upon', 'a'), 4)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ioc1xNjmnim-"
      },
      "source": [
        "# Applying Smoothing\n",
        "\n",
        "\n",
        "Assume additional training data in which each possible N-gram occurs exactly once and adjust estimates.\n",
        "\n",
        "> ### $ Probability(ngram) = \\frac{Count(ngram)+1}{ N\\, +\\, V} $\n",
        "\n",
        "N: Total number of N-grams <br>\n",
        "V: Number of unique N-grams\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDvK4mZ77CXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a6bd7d6-6d1c-4b2f-95ae-27c39ebabff8"
      },
      "source": [
        "type(biDist)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.probability.FreqDist"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "grh4sO0Yns4V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "5a96fc27-8a18-4dad-fe1e-d7bdaa5b6126"
      },
      "source": [
        "#You are to perform Add-1 smoothing here:\n",
        "#for unigram\n",
        "uniDict= dict(fdist)\n",
        "for key,value in uniDict.items():\n",
        "  value= (value+1)/(len(uni_processed)+len(fdist))\n",
        "  d= {key:value}\n",
        "  uniDict.update(d)\n",
        "#write similar code for bigram, trigram and fourgrams\n",
        "from collections import Counter\n",
        "#for bigrams\n",
        "biDict= dict(biDist)\n",
        "for key,value in biDict.items():\n",
        "  value= (value+1)/(len(bi_processed)+len(biDist))\n",
        "  d= {key:value}\n",
        "  biDict.update(d)\n",
        "#for trigrams\n",
        "triDict= dict(triDist)\n",
        "for key,value in triDict.items():\n",
        "  value= (value+1)/(len(tri_processed)+len(triDist))\n",
        "  d= {key:value}\n",
        "  triDict.update(d)\n",
        "#for fourgrams\n",
        "fourDict= dict(fourDist)\n",
        "for key,value in fourDict.items():\n",
        "  value= (value+1)/(len(four_processed)+len(fourDist))\n",
        "  d= {key:value}\n",
        "  fourDict.update(d)\n",
        "#Print top 10 unigram, bigram, trigram, fourgram after smoothing\n",
        "counter1=Counter(uniDict)\n",
        "common_element= counter1.most_common(10)\n",
        "print(common_element)\n",
        "counter1=Counter(biDict)\n",
        "common_element= counter1.most_common(10)\n",
        "print(common_element)\n",
        "counter1=Counter(triDict)\n",
        "common_element= counter1.most_common(10)\n",
        "print(common_element)\n",
        "counter1=Counter(fourDict)\n",
        "common_element= counter1.most_common(10)\n",
        "print(common_element)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('said', 0.0418281172379533), ('alice', 0.022056631892697468), ('little', 0.011922503725782414), ('went', 0.007948335817188276), ('one', 0.007848981619473424), ('like', 0.007550919026328862), ('would', 0.007153502235469449), ('thought', 0.006358668653750621), ('could', 0.006358668653750621), ('mock', 0.005663189269746647)]\n",
            "[(('said', 'the'), 0.007125323878358107), (('of', 'the'), 0.004602481930996863), (('in', 'a'), 0.0033751534160643666), (('the', 'and'), 0.0032387835810718667), (('and', 'the'), 0.002829674076094368), (('in', 'the'), 0.002727396699849993), (('to', 'the'), 0.0025228419473612438), (('at', 'the'), 0.0021137324423837448), (('as', 'she'), 0.00207963998363562), (('she', 'had'), 0.0019773626073912452)]\n",
            "[(('the', 'mock', 'turtle'), 0.0009417581447364549), (('said', 'the', 'mock'), 0.0005885988404602843), (('of', 'the', 'and'), 0.0005297389564142558), (('she', 'said', 'to'), 0.0005297389564142558), (('the', 'and', 'the'), 0.0005297389564142558), (('one', 'of', 'the'), 0.00047087907236822746), (('the', 'march', 'hare'), 0.00047087907236822746), (('said', 'the', 'and'), 0.0004414491303452132), (('the', 'said', 'the'), 0.0003825892462991848), (('out', 'of', 'the'), 0.0003531593042761706)]\n",
            "[(('as', 'well', 'as', 'she'), 0.00020631926432445179), (('in', 'a', 'tone', 'of'), 0.00020631926432445179), (('said', 'the', 'mock', 'turtle'), 0.00020631926432445179), (('the', 'poor', 'little', 'thing'), 0.00017684508370667296), (('a', 'minute', 'or', 'two'), 0.00017684508370667296), (('the', 'first', 'said', 'the'), 0.00017684508370667296), (('the', 'moral', 'of', 'that'), 0.00017684508370667296), (('she', 'did', 'not', 'like'), 0.00014737090308889413), (('did', 'not', 'like', 'to'), 0.00014737090308889413), (('she', 'came', 'upon', 'a'), 0.00014737090308889413)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k0GL40mQmmt4"
      },
      "source": [
        "### Predict the next word using statistical language modelling\n",
        "\n",
        "Using the above bigram, trigram, and fourgram models that you just experimented with, **predict the next word(top 5 probable) given the previous n(=2, 3, 4)-grams** for the sentences below.\n",
        "\n",
        "For str1, str2, you are to predict the next  2 possible word sequences using your trained smoothed models. <br> \n",
        "For example, for the string 'He looked very' the answers can be as below: \n",
        ">     (1) 'He looked very' *anxiouxly* \n",
        ">     (2) 'He looked very' *uncomfortable* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MBWKo5_Fmnbg",
        "colab": {}
      },
      "source": [
        "str1 = 'after that alice said the'\n",
        "str2 = 'alice felt so desperate that she was'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ext_nVn2mvZt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "efb91bbd-af1f-4d5f-e461-a3886b905e66"
      },
      "source": [
        "#write code\n",
        "print('For str1:')\n",
        "prob=0\n",
        "for key,value in biDict.items():\n",
        "  if(key[0]=='the' and value>prob):\n",
        "    prob=value;\n",
        "    word=key[1]\n",
        "print('bigram prediction: ',word)\n",
        "prob=0\n",
        "for key,value in triDict.items():\n",
        "  if(key[0]=='said' and key[1]=='the' and value>prob):\n",
        "    prob=value;\n",
        "    word=key[2]\n",
        "print('trigram prediction: ',word)\n",
        "prob=0\n",
        "for key,value in fourDict.items():\n",
        "  if(key[0]=='alice' and key[1]=='said' and key[2]=='the' and value>prob):\n",
        "    prob=value;\n",
        "    word=key[3]\n",
        "print('fourgram prediction: ',word)\n",
        "print('For str2:')\n",
        "prob=0\n",
        "for key,value in biDict.items():\n",
        "  if(key[0]=='was' and value>prob):\n",
        "    prob=value;\n",
        "    word=key[1]\n",
        "print('bigram prediction: ',word)\n",
        "prob=0\n",
        "for key,value in triDict.items():\n",
        "  if(key[0]=='she' and key[1]=='was' and value>prob):\n",
        "    prob=value;\n",
        "    word=key[2]\n",
        "print('trigram prediction: ',word)\n",
        "prob=0\n",
        "for key,value in fourDict.items():\n",
        "  if(key[0]=='that' and key[1]=='she' and key[2]=='was' and value>prob):\n",
        "    prob=value;\n",
        "    word=key[3]\n",
        "print('fourgram prediction: ',word)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For str1:\n",
            "bigram prediction:  and\n",
            "trigram prediction:  mock\n",
            "fourgram prediction:  mock\n",
            "For str2:\n",
            "bigram prediction:  a\n",
            "trigram prediction:  now\n",
            "fourgram prediction:  now\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YBGqqphFvlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}